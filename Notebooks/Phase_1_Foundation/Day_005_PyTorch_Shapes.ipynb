{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6uFAPcKkYXXDxzmiMqG7d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Signeemmanuel/research-journey-2025/blob/master/Notebooks/Phase_1_Foundation/Day_005_PyTorch_Shapes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0y2MpFZdURx",
        "outputId": "a61e7949-f411-4ef4-8fed-c3ad1c50153b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch version 2.9.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Pytorch version {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmvLb6uIeOjT",
        "outputId": "98936ed7-a1d6-459c-848d-bada3f2f432e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ec8db7496d0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Anatomy of tensors\n",
        "Creating tensors and inspecting their properties."
      ],
      "metadata": {
        "id": "NCHPoZildonN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 3-Rank tensor (eg. represent a simplified image: Channel, Height, Width)\n",
        "x = torch.rand(3, 4, 4)\n",
        "\n",
        "print(\"Tensor x:\\n\", x)\n",
        "print(\"\\n-----Properties-----\\n\")\n",
        "print(f\"Shape: {x.shape}\")\n",
        "print(f\"Data Type: {x.dtype}\")\n",
        "print(f\"Device: {x.device}\")\n",
        "print(f\"Total Elements: {x.numel()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy0ahCCsdlOF",
        "outputId": "89a4e213-457e-4782-9fcd-ede72f8537a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor x:\n",
            " tensor([[[0.8823, 0.9150, 0.3829, 0.9593],\n",
            "         [0.3904, 0.6009, 0.2566, 0.7936],\n",
            "         [0.9408, 0.1332, 0.9346, 0.5936],\n",
            "         [0.8694, 0.5677, 0.7411, 0.4294]],\n",
            "\n",
            "        [[0.8854, 0.5739, 0.2666, 0.6274],\n",
            "         [0.2696, 0.4414, 0.2969, 0.8317],\n",
            "         [0.1053, 0.2695, 0.3588, 0.1994],\n",
            "         [0.5472, 0.0062, 0.9516, 0.0753]],\n",
            "\n",
            "        [[0.8860, 0.5832, 0.3376, 0.8090],\n",
            "         [0.5779, 0.9040, 0.5547, 0.3423],\n",
            "         [0.6343, 0.3644, 0.7104, 0.9464],\n",
            "         [0.7890, 0.2814, 0.7886, 0.5895]]])\n",
            "\n",
            "-----Properties-----\n",
            "\n",
            "Shape: torch.Size([3, 4, 4])\n",
            "Data Type: torch.float32\n",
            "Device: cpu\n",
            "Total Elements: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View vs. Reshape\n",
        "Understanding memory layout"
      ],
      "metadata": {
        "id": "HKUaRtARgC7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to flatten the image pixel per channel\n",
        "# Target (3, 16)\n",
        "\n",
        "# METHOD 1: View\n",
        "# View works only on contigous tensors (tensors stored in a single block of memory)\n",
        "z_view = x.view(3, 16)\n",
        "print(f\"View Shape: {z_view.shape}\")\n",
        "\n",
        "# METHOD 2: reshape\n",
        "# reshape works on any tensors. it will copy data if neccessary to make it contiguous\n",
        "z_reshape = x.reshape(3, 16)\n",
        "print(f\"Reshape shape: {z_reshape.shape}\")\n",
        "\n",
        "# Why use view()?\n",
        "# view() is faster because it never copies data. It just changes the metadata (stride).\n",
        "# If you try to view() a non-contiguous tensor, PyTorch throws an error."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H-4Zm9Wfeb_",
        "outputId": "65c0c1a9-8457-4ee2-a6be-e496bfdf19ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View Shape: torch.Size([3, 16])\n",
            "Reshape shape: torch.Size([3, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Permute (Crucial for Computer Vision)\n",
        "Concept changing dimension order (NHWC vs NCHW)"
      ],
      "metadata": {
        "id": "1i5L4SIliJqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy batch of images\n",
        "# N (Batch) = 2\n",
        "# C (Channels) = 3\n",
        "# H (Height) = 224\n",
        "# W (Width) = 224\n",
        "batch_images = torch.zeros(2, 3, 244, 244)\n",
        "print(f\"Original Shape: {batch_images.shape}\")\n",
        "\n",
        "# PROBLEM: Matplotlib and OpenCV expect images as (Height, Width, Channels).\n",
        "# We need to move dim 1 (C) to the end\n",
        "\n",
        "# ACTION: permute dimension (0, 2, 3, 1)\n",
        "# keep (0) N at pos 0\n",
        "# Move (1) C at pos 3\n",
        "# Move (2) H at pos 1\n",
        "# Move (3) W at pos 2\n",
        "images_for_plotting = batch_images.permute(0, 2, 3, 1)\n",
        "print(f\"Permuted shape (OpenCV Format): {images_for_plotting.shape}\")\n",
        "\n",
        "# WARNING: Permute breaks continuity\n",
        "print(f\"Is Contiguous: {images_for_plotting.is_contiguous()}\")\n",
        "\n",
        "# # If we try to view this now it will crash\n",
        "try:\n",
        "  images_for_plotting.view(2, -1) # Try to flatten\n",
        "except RuntimeError as e:\n",
        "  print(f\"\\n Expected Error: {e}\")\n",
        "\n",
        "\n",
        "# FIX: Cal .contiguous() before .view()\n",
        "flattened = images_for_plotting.contiguous().view(2, -1)\n",
        "print(f\"Fixed shape: {flattened.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8uNmGCxhAtm",
        "outputId": "fa14f369-d03f-4e05-d720-3f28b222ac5a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Shape: torch.Size([2, 3, 244, 244])\n",
            "Permuted shape (OpenCV Format): torch.Size([2, 244, 244, 3])\n",
            "Is Contiguous: False\n",
            "\n",
            " Expected Error: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
            "Fixed shape: torch.Size([2, 178608])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Squeeze and Unsqueeze\n",
        "Concepts: Adding or removing dimensions of size 1(Broadcasting prep)."
      ],
      "metadata": {
        "id": "ggkId7r3oDIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsqueeze: Add a dimension (often for batching)\n",
        "img = torch.zeros(3, 224, 224) # Single image\n",
        "print(f\"Single Image shape: {img.shape}\")\n",
        "\n",
        "# The Model expects (Batch, C, H, W). We need to add batch dim at dim index 0.\n",
        "img_batch = img.unsqueeze(1)\n",
        "print(f\"Batch Image shape: {img_batch.shape}\")\n",
        "\n",
        "# Squeeze: Remove dimension of size 1\n",
        "# Useful when a model outputs shape (Batch, 1) but you need batch for loss calculation\n",
        "model_output = torch.zeros(10, 1)\n",
        "print(f\"Model output: {model_output.shape}\")\n",
        "\n",
        "squeeze_output = model_output.squeeze(1) # Remove dim at index 1\n",
        "print(f\"Squeeze ouput: {squeeze_output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hBNcUhGjGUT",
        "outputId": "435e989a-0a09-4251-99b8-797951554563"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Image shape: torch.Size([3, 224, 224])\n",
            "Batch Image shape: torch.Size([3, 1, 224, 224])\n",
            "Model output: torch.Size([10, 1])\n",
            "Squeeze ouput: torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Challenge (Flattening Linear Layer)\n",
        "This is exactly what happens inside CNN classifyer head"
      ],
      "metadata": {
        "id": "JiYJOPP1r7VV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate the output of a convolutional layer\n",
        "# Batch = 128, Channel = 64, FeatureMap = 4x4\n",
        "\n",
        "conv_output = torch.rand(128, 64, 4, 4)\n",
        "\n",
        "# We want to feed this into a Linear Layer (Dense).\n",
        "# Linear layers expect (Batch_Size, Input_Features).\n",
        "# We must flatten (64, 4, 4) into a single vector of size 64*4*4 = 1024.\n",
        "\n",
        "# Correct way:\n",
        "# Keep dim 0 (Batch) as is.\n",
        "# Flatten everything else (-1 tells PyTorch to figure out the number).\n",
        "flat_output = conv_output.view(128, -1)\n",
        "\n",
        "print(f\"Conv Output: {conv_output.shape}\")\n",
        "print(f\"Linear Input: {flat_output.shape}\")\n",
        "\n",
        "assert flat_output.shape == (128, 1024)\n",
        "print(\"Transformation correct.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohrzvnEGqeF7",
        "outputId": "339fda44-9c4f-40e6-a2d8-19af900678b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv Output: torch.Size([128, 64, 4, 4])\n",
            "Linear Input: torch.Size([128, 1024])\n",
            "Transformation correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7myyCy2psiGZ"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}